{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 54) (581012,)\n"
     ]
    }
   ],
   "source": [
    "covertype = fetch_covtype() # https://archive.ics.uci.edu/dataset/31/covertype\n",
    "X = covertype.data\n",
    "y = covertype.target\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MachineLearningSkoltech\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "d:\\MachineLearningSkoltech\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Metrics:\n",
      "Accuracy: 0.4953242610611346\n",
      "Precision: 0.6099244478930624\n",
      "Recall: 0.4953242610611346\n",
      "Weighted F1 Score: 0.5239976538323496\n",
      "\n",
      "Gradient Boosting Classifier Metrics:\n",
      "Accuracy: 0.7706363594639251\n",
      "Precision: 0.7698504844584944\n",
      "Recall: 0.7706363594639251\n",
      "F1 Score: 0.7668495633578172\n",
      "\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.75      0.75     63556\n",
      "           2       0.78      0.82      0.80     85078\n",
      "           3       0.77      0.83      0.80     10638\n",
      "           4       0.79      0.74      0.77       795\n",
      "           5       0.75      0.23      0.36      2941\n",
      "           6       0.66      0.48      0.55      5227\n",
      "           7       0.87      0.70      0.77      6069\n",
      "\n",
      "    accuracy                           0.77    174304\n",
      "   macro avg       0.77      0.65      0.69    174304\n",
      "weighted avg       0.77      0.77      0.77    174304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "adaboost_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier on the training data\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data using AdaBoost\n",
    "y_pred_adaboost = adaboost_clf.predict(X_test)\n",
    "\n",
    "# Calculate various metrics for AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "precision_adaboost = precision_score(y_test, y_pred_adaboost, average='weighted')\n",
    "recall_adaboost = recall_score(y_test, y_pred_adaboost, average='weighted')\n",
    "f1_adaboost = f1_score(y_test, y_pred_adaboost, average='weighted')\n",
    "\n",
    "# Print the metrics for AdaBoost\n",
    "print(\"AdaBoost Classifier Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_adaboost)\n",
    "print(\"Precision:\", precision_adaboost)\n",
    "print(\"Recall:\", recall_adaboost)\n",
    "print(\"Weighted F1 Score:\", f1_adaboost)\n",
    "\n",
    "gradientboost_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting classifier on the training data\n",
    "gradientboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data using Gradient Boosting\n",
    "y_pred_gradientboost = gradientboost_clf.predict(X_test)\n",
    "\n",
    "# Calculate various metrics for Gradient Boosting\n",
    "accuracy_gradientboost = accuracy_score(y_test, y_pred_gradientboost)\n",
    "precision_gradientboost = precision_score(y_test, y_pred_gradientboost, average='weighted')\n",
    "recall_gradientboost = recall_score(y_test, y_pred_gradientboost, average='weighted')\n",
    "f1_gradientboost = f1_score(y_test, y_pred_gradientboost, average='weighted')\n",
    "\n",
    "# Print the metrics for Gradient Boosting\n",
    "print(\"\\nGradient Boosting Classifier Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_gradientboost)\n",
    "print(\"Precision:\", precision_gradientboost)\n",
    "print(\"Recall:\", recall_gradientboost)\n",
    "print(\"F1 Score:\", f1_gradientboost)\n",
    "\n",
    "# Classification report for Gradient Boosting\n",
    "print(\"\\nClassification Report for Gradient Boosting:\")\n",
    "print(classification_report(y_test, y_pred_gradientboost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.65      0.62     63556\n",
      "           2       0.75      0.43      0.55     85078\n",
      "           3       0.27      0.36      0.31     10638\n",
      "           4       0.00      0.00      0.00       795\n",
      "           5       0.29      0.02      0.04      2941\n",
      "           6       0.10      0.47      0.16      5227\n",
      "           7       0.12      0.34      0.18      6069\n",
      "\n",
      "    accuracy                           0.50    174304\n",
      "   macro avg       0.30      0.32      0.27    174304\n",
      "weighted avg       0.61      0.50      0.52    174304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MachineLearningSkoltech\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\MachineLearningSkoltech\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\MachineLearningSkoltech\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_adaboost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for AdaBoost:\n",
      "[[41076 10729   930     0    10   843  9968]\n",
      " [23650 36838  6561     0   161 13278  4590]\n",
      " [   51   281  3865     0     1  6440     0]\n",
      " [    0     0    77     0     0   718     0]\n",
      " [  552  1245   217     0    71   856     0]\n",
      " [   25   116  2644     0     0  2442     0]\n",
      " [ 4012    12     0     0     0     0  2045]]\n",
      "\n",
      "Confusion Matrix for Gradient Boosting:\n",
      "[[47454 15421    24     0    29    24   604]\n",
      " [13468 70057   779     1   184   536    53]\n",
      " [    0  1002  8801   112     9   714     0]\n",
      " [    0     0   183   588     0    24     0]\n",
      " [   33  2128    83     0   691     6     0]\n",
      " [    0  1068  1610    40     4  2505     0]\n",
      " [ 1799    41     0     0     0     0  4229]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix for AdaBoost\n",
    "conf_matrix_adaboost = confusion_matrix(y_test, y_pred_adaboost)\n",
    "print(\"\\nConfusion Matrix for AdaBoost:\")\n",
    "print(conf_matrix_adaboost)\n",
    "\n",
    "# Compute confusion matrix for Gradient Boosting\n",
    "conf_matrix_gradientboost = confusion_matrix(y_test, y_pred_gradientboost)\n",
    "print(\"\\nConfusion Matrix for Gradient Boosting:\")\n",
    "print(conf_matrix_gradientboost)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
